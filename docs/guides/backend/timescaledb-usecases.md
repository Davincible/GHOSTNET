# TimescaleDB Real-World Guide 2026
## Decision Frameworks, Use Cases, and Edge Cases

*Companion to the TimescaleDB Master Guide | January 2026*

---

## Table of Contents

1. [The Hypertable Decision Framework](#1-the-hypertable-decision-framework)
2. [When NOT to Use Hypertables](#2-when-not-to-use-hypertables)
3. [Real-World Use Cases](#3-real-world-use-cases)
4. [Tricky Hybrid Scenarios](#4-tricky-hybrid-scenarios)
5. [Multi-Tenant Architecture Patterns](#5-multi-tenant-architecture-patterns)
6. [TimescaleDB vs Alternatives Decision Guide](#6-timescaledb-vs-alternatives-decision-guide)
7. [Migration Decision Trees](#7-migration-decision-trees)
8. [Performance Optimization Decision Framework](#8-performance-optimization-decision-framework)

---

## 1. The Hypertable Decision Framework

### Master Decision Diagram

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Do you have timestamped   â”‚
                    â”‚          data?              â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                             â”‚
                   YES                           NO
                    â”‚                             â”‚
                    â–¼                             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Is data primarily appended   â”‚    â”‚ Use regular Postgres â”‚
    â”‚  (not frequently updated)?    â”‚    â”‚       tables         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚
       YES                     NO
        â”‚                       â”‚
        â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Expected rows >   â”‚   â”‚ Consider regular tables    â”‚
â”‚ 10 million?       â”‚   â”‚ with UPDATE/DELETE support â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ OR carefully plan for      â”‚
        â”‚               â”‚ decompression overhead     â”‚
  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚           â”‚
 YES         NO
  â”‚           â”‚
  â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USE     â”‚  â”‚  Still consider hypertables if:      â”‚
â”‚HYPER-   â”‚  â”‚  â€¢ Need compression (>90% savings)   â”‚
â”‚TABLE    â”‚  â”‚  â€¢ Need time-based retention         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â€¢ Need continuous aggregates        â”‚
             â”‚  â€¢ Data will grow over time          â”‚
             â”‚  Otherwise: regular Postgres tables  â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The 5-Question Hypertable Test

Answer these questions about your data:

| Question | Yes â†’ Points | No â†’ Points |
|----------|--------------|-------------|
| 1. Is time (or UUIDv7) a natural query filter? | +3 | -1 |
| 2. Will you have >10 million rows? | +2 | 0 |
| 3. Is data mostly append-only? | +2 | -1 |
| 4. Do you need time-based data retention? | +2 | 0 |
| 5. Are you doing time-range aggregations? | +2 | 0 |

**Scoring:**
- **7+ points**: Definitely use hypertables
- **4-6 points**: Hypertables likely beneficial
- **1-3 points**: Evaluate carefully, could go either way
- **0 or negative**: Stick with regular PostgreSQL tables

### Feature-Based Decision Matrix

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Feature Need           â”‚Hypertable â”‚ Regular Table â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Automatic time partitioning      â”‚    âœ…     â”‚      âŒ       â”‚
â”‚ Columnar compression (90%+)      â”‚    âœ…     â”‚      âŒ       â”‚
â”‚ Continuous aggregates            â”‚    âœ…     â”‚      âŒ       â”‚
â”‚ Automatic chunk retention        â”‚    âœ…     â”‚      âŒ       â”‚
â”‚ Time-bucket queries              â”‚    âœ…     â”‚   ğŸŸ¡ Manual   â”‚
â”‚ Tiered storage (hot/cold)        â”‚    âœ…     â”‚      âŒ       â”‚
â”‚ SkipScan for DISTINCT            â”‚    âœ…     â”‚      âŒ       â”‚
â”‚ Foreign key TO this table        â”‚    âŒ     â”‚      âœ…       â”‚
â”‚ Frequent single-row UPDATEs      â”‚    ğŸŸ¡     â”‚      âœ…       â”‚
â”‚ Cross-partition UPSERTs          â”‚    âŒ     â”‚      âœ…       â”‚
â”‚ Table inheritance                â”‚    âŒ     â”‚      âœ…       â”‚
â”‚ Very small datasets (<1M rows)   â”‚    ğŸŸ¡     â”‚      âœ…       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… = Excellent support  ğŸŸ¡ = Possible with caveats  âŒ = Not supported
```

---

## 2. When NOT to Use Hypertables

### Hard Constraints (Never Use Hypertables If...)

#### 1. Foreign Key References TO the Table

```sql
-- âŒ THIS WILL NOT WORK
CREATE TABLE events (
    id SERIAL PRIMARY KEY,
    event_time TIMESTAMPTZ NOT NULL,
    data JSONB
);
SELECT create_hypertable('events', 'event_time');

-- This fails or misbehaves:
CREATE TABLE event_comments (
    id SERIAL PRIMARY KEY,
    event_id INT REFERENCES events(id)  -- âŒ FK to hypertable!
);
```

**Solution Pattern**: Use a regular table for the "referenced" entity, hypertable for time-series data:

```sql
-- âœ… CORRECT PATTERN
-- Regular table for entities
CREATE TABLE devices (
    id SERIAL PRIMARY KEY,
    name TEXT NOT NULL,
    metadata JSONB
);

-- Hypertable for time-series (FK FROM hypertable is OK)
CREATE TABLE device_metrics (
    time TIMESTAMPTZ NOT NULL,
    device_id INT REFERENCES devices(id),  -- âœ… FK from hypertable is fine
    cpu_usage FLOAT,
    memory_usage FLOAT
);
SELECT create_hypertable('device_metrics', 'time');
```

#### 2. Cross-Chunk UPDATE/UPSERT Operations

```sql
-- âŒ NOT SUPPORTED: UPDATE that moves data between chunks
UPDATE metrics 
SET time = time + INTERVAL '30 days'  -- Changes partition!
WHERE device_id = 'sensor-1';

-- Error: UPDATE statements that move values between 
-- partitions (chunks) are not supported.
```

**Solution**: Delete and re-insert, or design schema to avoid cross-chunk moves.

#### 3. Unique Constraints Without Time Column

```sql
-- âŒ THIS WILL FAIL
CREATE TABLE events (
    event_id UUID PRIMARY KEY,  -- Must include time!
    time TIMESTAMPTZ NOT NULL,
    data JSONB
);
SELECT create_hypertable('events', 'time');

-- Error: Cannot create unique index without partition column
```

**Solution**: Include time in the unique constraint:

```sql
-- âœ… CORRECT: Include partition column
CREATE TABLE events (
    event_id UUID NOT NULL,
    time TIMESTAMPTZ NOT NULL,
    data JSONB,
    PRIMARY KEY (event_id, time)  -- Time included
);
SELECT create_hypertable('events', 'time');
```

### Soft Constraints (Carefully Evaluate)

#### Small Datasets (<1 Million Rows)

For small datasets, hypertable overhead may not be justified:

```
Dataset Size        Recommendation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
< 100K rows         Regular table (unless growth expected)
100K - 1M rows      Consider compression benefits
1M - 10M rows       Hypertables beneficial
> 10M rows          Hypertables strongly recommended
```

**Exception**: Even small datasets benefit from hypertables if you need:
- Automatic data retention (e.g., delete data older than 90 days)
- Continuous aggregates for dashboard performance
- Future growth is expected

#### High-Frequency Single-Row Lookups

```sql
-- If 90%+ of queries are point lookups like this:
SELECT * FROM events WHERE event_id = 'abc-123';

-- Regular tables with proper indexes may be faster
-- Hypertables optimize for time-range scans, not point lookups
```

**Benchmark first**: The Hypercore engine has improved point-query performance significantly in 2.20+, but always test your specific workload.

#### Heavily UPDATE-Heavy Workloads

```sql
-- If you're doing this millions of times:
UPDATE metrics SET status = 'processed' WHERE id = 123;
```

Compressed chunks require decompression for updates. Consider:
- Keeping a longer compression delay (7+ days)
- Separating mutable and immutable data into different tables
- Using an "events" pattern instead of mutable state

### The "Partial Fit" Decision Tree

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Your data partially fits TimescaleDB    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                                   â”‚
     Some data is              All data has timestamps
     non-time-series           but mixed access patterns
          â”‚                                   â”‚
          â–¼                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HYBRID APPROACH:    â”‚        â”‚ SINGLE-TABLE WITH CAREFUL   â”‚
â”‚ â€¢ Regular tables    â”‚        â”‚ COMPRESSION SETTINGS:       â”‚
â”‚   for entities      â”‚        â”‚ â€¢ Longer compress_after     â”‚
â”‚ â€¢ Hypertables for   â”‚        â”‚ â€¢ Strategic segmentby       â”‚
â”‚   time-series data  â”‚        â”‚ â€¢ Consider separate tables  â”‚
â”‚ â€¢ Join as needed    â”‚        â”‚   for "hot" mutable data    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Real-World Use Cases

### Use Case 1: IoT Sensor Monitoring

**Scenario**: 10,000 sensors sending data every second, 90-day retention, real-time dashboards.

```sql
-- Metadata table (regular PostgreSQL)
CREATE TABLE sensors (
    sensor_id TEXT PRIMARY KEY,
    location TEXT NOT NULL,
    sensor_type TEXT NOT NULL,
    installed_at TIMESTAMPTZ DEFAULT NOW(),
    metadata JSONB
);

-- Time-series data (hypertable)
CREATE TABLE sensor_readings (
    time TIMESTAMPTZ NOT NULL,
    sensor_id TEXT NOT NULL REFERENCES sensors(sensor_id),
    temperature FLOAT,
    humidity FLOAT,
    pressure FLOAT,
    battery_level FLOAT
) WITH (
    timescaledb.hypertable,
    timescaledb.partition_column = 'time',
    timescaledb.chunk_time_interval = '1 day',
    timescaledb.segmentby = 'sensor_id',
    timescaledb.orderby = 'time DESC',
    timescaledb.compress_after = '7 days'
);

-- Essential indexes
CREATE INDEX ON sensor_readings (sensor_id, time DESC);

-- Retention policy
SELECT add_retention_policy('sensor_readings', INTERVAL '90 days');

-- Continuous aggregate for dashboards
CREATE MATERIALIZED VIEW sensor_hourly
WITH (timescaledb.continuous) AS
SELECT 
    time_bucket('1 hour', time) AS bucket,
    sensor_id,
    AVG(temperature) AS avg_temp,
    MIN(temperature) AS min_temp,
    MAX(temperature) AS max_temp,
    AVG(humidity) AS avg_humidity,
    COUNT(*) AS reading_count
FROM sensor_readings
GROUP BY bucket, sensor_id
WITH NO DATA;

SELECT add_continuous_aggregate_policy('sensor_hourly',
    start_offset => INTERVAL '3 hours',
    end_offset => INTERVAL '1 hour',
    schedule_interval => INTERVAL '1 hour'
);
```

**Architecture Diagram**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Sensors   â”‚â”€â”€â”€â”€â–¶â”‚           TimescaleDB                       â”‚
â”‚  (10,000)   â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚   sensors   â”‚â—„â”€â”€â”€â”‚ sensor_readings  â”‚   â”‚
                    â”‚  â”‚  (regular)  â”‚ FK â”‚   (hypertable)   â”‚   â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                    â”‚                              â”‚              â”‚
                    â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
                    â”‚                     â”‚  sensor_hourly   â”‚   â”‚
                    â”‚                     â”‚ (continuous agg) â”‚   â”‚
                    â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                       â”‚                   â”‚
                    â–¼                       â–¼                   â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚ Grafana  â”‚           â”‚  Alerts  â”‚        â”‚   API    â”‚
             â”‚Dashboard â”‚           â”‚ (recent) â”‚        â”‚(raw data)â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Use Case 2: Financial Tick Data & OHLC Candles

**Scenario**: High-frequency trading data, 10M+ ticks/day, need OHLC candles at multiple timeframes.

```sql
-- Raw tick data
CREATE TABLE ticks (
    time TIMESTAMPTZ NOT NULL,
    symbol TEXT NOT NULL,
    price NUMERIC(20,8) NOT NULL,
    volume NUMERIC(20,8) NOT NULL
) WITH (
    timescaledb.hypertable,
    timescaledb.partition_column = 'time',
    timescaledb.chunk_time_interval = '4 hours',
    timescaledb.segmentby = 'symbol',
    timescaledb.orderby = 'time ASC',
    timescaledb.compress_after = '1 day'
);

-- Index for symbol queries
CREATE INDEX ON ticks (symbol, time DESC);

-- 1-minute OHLC candles (continuous aggregate)
CREATE MATERIALIZED VIEW ohlc_1m
WITH (timescaledb.continuous) AS
SELECT 
    time_bucket('1 minute', time) AS bucket,
    symbol,
    FIRST(price, time) AS open,
    MAX(price) AS high,
    MIN(price) AS low,
    LAST(price, time) AS close,
    SUM(volume) AS volume
FROM ticks
GROUP BY bucket, symbol
WITH NO DATA;

SELECT add_continuous_aggregate_policy('ohlc_1m',
    start_offset => INTERVAL '10 minutes',
    end_offset => INTERVAL '1 minute',
    schedule_interval => INTERVAL '1 minute'
);

-- Hierarchical aggregation: 1-hour OHLC from 1-minute
CREATE MATERIALIZED VIEW ohlc_1h
WITH (timescaledb.continuous) AS
SELECT 
    time_bucket('1 hour', bucket) AS bucket,
    symbol,
    FIRST(open, bucket) AS open,
    MAX(high) AS high,
    MIN(low) AS low,
    LAST(close, bucket) AS close,
    SUM(volume) AS volume
FROM ohlc_1m
GROUP BY time_bucket('1 hour', bucket), symbol
WITH NO DATA;

SELECT add_continuous_aggregate_policy('ohlc_1h',
    start_offset => INTERVAL '4 hours',
    end_offset => INTERVAL '1 hour',
    schedule_interval => INTERVAL '1 hour'
);

-- Daily OHLC from hourly
CREATE MATERIALIZED VIEW ohlc_1d
WITH (timescaledb.continuous) AS
SELECT 
    time_bucket('1 day', bucket) AS bucket,
    symbol,
    FIRST(open, bucket) AS open,
    MAX(high) AS high,
    MIN(low) AS low,
    LAST(close, bucket) AS close,
    SUM(volume) AS volume
FROM ohlc_1h
GROUP BY time_bucket('1 day', bucket), symbol
WITH NO DATA;
```

**Retention Strategy**:
```sql
-- Raw ticks: 7 days
SELECT add_retention_policy('ticks', INTERVAL '7 days');

-- 1-minute: 30 days
SELECT add_retention_policy('ohlc_1m', INTERVAL '30 days');

-- 1-hour: 1 year
SELECT add_retention_policy('ohlc_1h', INTERVAL '1 year');

-- 1-day: Forever (no retention policy)
```

### Use Case 3: Application Metrics & Observability

**Scenario**: Prometheus-like metrics storage with labels, long-term retention, downsampling.

```sql
-- Metrics with labels (tags)
CREATE TABLE metrics (
    time TIMESTAMPTZ NOT NULL,
    metric_name TEXT NOT NULL,
    labels JSONB NOT NULL,  -- {"service": "api", "instance": "host1"}
    value DOUBLE PRECISION NOT NULL
) WITH (
    timescaledb.hypertable,
    timescaledb.partition_column = 'time',
    timescaledb.chunk_time_interval = '1 day',
    timescaledb.compress_after = '2 days'
);

-- Compression settings for high-cardinality labels
ALTER TABLE metrics SET (
    timescaledb.compress_segmentby = 'metric_name',
    timescaledb.compress_orderby = 'time DESC'
);

-- GIN index for label queries
CREATE INDEX ON metrics USING GIN (labels);

-- Composite index for metric + time
CREATE INDEX ON metrics (metric_name, time DESC);

-- 5-minute rollup for dashboards
CREATE MATERIALIZED VIEW metrics_5m
WITH (timescaledb.continuous) AS
SELECT 
    time_bucket('5 minutes', time) AS bucket,
    metric_name,
    labels,
    AVG(value) AS avg_value,
    MIN(value) AS min_value,
    MAX(value) AS max_value,
    COUNT(*) AS sample_count
FROM metrics
GROUP BY bucket, metric_name, labels
WITH NO DATA;

-- Query patterns
-- Recent data with specific labels:
SELECT time, value 
FROM metrics 
WHERE metric_name = 'http_requests_total'
  AND labels @> '{"service": "api"}'
  AND time > NOW() - INTERVAL '1 hour'
ORDER BY time DESC;

-- Aggregated dashboard data:
SELECT bucket, avg_value
FROM metrics_5m
WHERE metric_name = 'cpu_usage'
  AND labels @> '{"host": "web-01"}'
  AND bucket > NOW() - INTERVAL '24 hours';
```

### Use Case 4: Event Sourcing / Audit Log

**Scenario**: Immutable event log for compliance, need to query events by entity and time.

```sql
-- Event store (hypertable)
CREATE TABLE events (
    event_id UUID NOT NULL DEFAULT gen_random_uuid(),
    event_time TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    event_type TEXT NOT NULL,
    aggregate_type TEXT NOT NULL,  -- 'Order', 'User', etc.
    aggregate_id UUID NOT NULL,
    payload JSONB NOT NULL,
    metadata JSONB DEFAULT '{}',
    PRIMARY KEY (event_id, event_time)  -- Must include time!
) WITH (
    timescaledb.hypertable,
    timescaledb.partition_column = 'event_time',
    timescaledb.chunk_time_interval = '1 week'
);

-- Compression (events are immutable - compress aggressively)
ALTER TABLE events SET (
    timescaledb.compress_segmentby = 'aggregate_type',
    timescaledb.compress_orderby = 'event_time DESC',
    timescaledb.compress_after = '1 day'
);

-- Index for aggregate queries
CREATE INDEX ON events (aggregate_type, aggregate_id, event_time DESC);

-- Index for event type queries
CREATE INDEX ON events (event_type, event_time DESC);

-- Query: Get all events for an order
SELECT * FROM events
WHERE aggregate_type = 'Order'
  AND aggregate_id = 'order-uuid-here'
ORDER BY event_time ASC;

-- Query: Get recent events of a type
SELECT * FROM events
WHERE event_type = 'OrderCreated'
  AND event_time > NOW() - INTERVAL '24 hours'
ORDER BY event_time DESC;

-- Retention: Keep raw events for 2 years
SELECT add_retention_policy('events', INTERVAL '2 years');
```

**Gotcha**: Events are immutable, but you can't have a simple `event_id PRIMARY KEY`. You must include the partition column.

### Use Case 5: Multi-Tenant SaaS Analytics

**Scenario**: B2B SaaS with 1000s of tenants, each tenant's data must be isolated, varying data volumes per tenant.

```sql
-- Tenant metadata (regular table)
CREATE TABLE tenants (
    tenant_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name TEXT NOT NULL,
    plan TEXT NOT NULL DEFAULT 'free',
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Per-tenant analytics (hypertable)
CREATE TABLE tenant_events (
    time TIMESTAMPTZ NOT NULL,
    tenant_id UUID NOT NULL REFERENCES tenants(tenant_id),
    event_type TEXT NOT NULL,
    user_id UUID,
    properties JSONB,
    PRIMARY KEY (tenant_id, time, event_type)
) WITH (
    timescaledb.hypertable,
    timescaledb.partition_column = 'time',
    timescaledb.chunk_time_interval = '1 day'
);

-- CRITICAL: Segmentby tenant for compression
ALTER TABLE tenant_events SET (
    timescaledb.compress_segmentby = 'tenant_id',
    timescaledb.compress_orderby = 'time DESC',
    timescaledb.compress_after = '7 days'
);

-- Index for tenant isolation
CREATE INDEX ON tenant_events (tenant_id, time DESC);
CREATE INDEX ON tenant_events (tenant_id, event_type, time DESC);

-- Row-Level Security for tenant isolation
ALTER TABLE tenant_events ENABLE ROW LEVEL SECURITY;

CREATE POLICY tenant_isolation ON tenant_events
    USING (tenant_id = current_setting('app.current_tenant')::UUID);

-- Usage: Set tenant context before queries
SET app.current_tenant = 'tenant-uuid-here';
SELECT * FROM tenant_events WHERE time > NOW() - INTERVAL '7 days';

-- Per-tenant continuous aggregates (with tenant_id in GROUP BY)
CREATE MATERIALIZED VIEW tenant_daily_stats
WITH (timescaledb.continuous) AS
SELECT 
    time_bucket('1 day', time) AS day,
    tenant_id,
    event_type,
    COUNT(*) AS event_count,
    COUNT(DISTINCT user_id) AS unique_users
FROM tenant_events
GROUP BY day, tenant_id, event_type
WITH NO DATA;
```

**Multi-Tenant Decision Points**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Multi-Tenant Isolation Strategy                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ How many        â”‚     â”‚ < 100 tenants + high isolation:   â”‚ â”‚
â”‚  â”‚ tenants?        â”‚â”€â”€â”€â”€â–¶â”‚ Consider database-per-tenant      â”‚ â”‚
â”‚  â”‚                 â”‚     â”‚                                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ > 100 tenants:                    â”‚ â”‚
â”‚           â”‚              â”‚ Shared database + RLS             â”‚ â”‚
â”‚           â–¼              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚ Data volume     â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ varies greatly? â”‚â”€â”€â”€â”€â–¶â”‚ Yes: Use tenant_id as segmentby   â”‚ â”‚
â”‚  â”‚                 â”‚     â”‚ (each tenant compressed separately)â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                                                     â”‚
â”‚           â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Need per-tenant â”‚     â”‚ Add retention policies per-tenant â”‚ â”‚
â”‚  â”‚ retention?      â”‚â”€â”€â”€â”€â–¶â”‚ (requires custom job or filtering)â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Tricky Hybrid Scenarios

### Scenario A: Mixed Time-Series and Transactional Data

**Problem**: Application needs both fast transactional updates AND time-series analytics.

**Solution**: Hybrid architecture with separate tables

```sql
-- Transactional data (regular table)
CREATE TABLE orders (
    order_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    customer_id UUID NOT NULL,
    status TEXT NOT NULL DEFAULT 'pending',
    total_amount NUMERIC(10,2),
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Order status changes (hypertable for analytics)
CREATE TABLE order_events (
    time TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    order_id UUID NOT NULL,
    event_type TEXT NOT NULL,  -- 'created', 'paid', 'shipped', 'delivered'
    previous_status TEXT,
    new_status TEXT,
    metadata JSONB
) WITH (
    timescaledb.hypertable,
    timescaledb.partition_column = 'time'
);

-- Trigger to capture status changes
CREATE OR REPLACE FUNCTION log_order_status_change()
RETURNS TRIGGER AS $$
BEGIN
    IF OLD.status IS DISTINCT FROM NEW.status THEN
        INSERT INTO order_events (order_id, event_type, previous_status, new_status)
        VALUES (NEW.order_id, 'status_change', OLD.status, NEW.status);
    END IF;
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER order_status_change
BEFORE UPDATE ON orders
FOR EACH ROW EXECUTE FUNCTION log_order_status_change();
```

### Scenario B: High-Cardinality Tags Problem

**Problem**: Metrics with millions of unique tag combinations.

**Anti-pattern** (causes tiny compressed segments):
```sql
-- âŒ DON'T DO THIS
ALTER TABLE metrics SET (
    timescaledb.compress_segmentby = 'metric_name, host, container_id, pod_name'
);
-- Results in millions of tiny segments with poor compression
```

**Solution**: Use orderby instead of segmentby for high-cardinality columns:

```sql
-- âœ… CORRECT APPROACH
ALTER TABLE metrics SET (
    timescaledb.compress_segmentby = 'metric_name',  -- Low cardinality
    timescaledb.compress_orderby = 'host, time DESC'  -- High cardinality in orderby
);

-- Also enable bloom filters for point queries on high-cardinality columns
ALTER TABLE metrics SET (
    timescaledb.compress_index = 'bloom("container_id"), minmax("time")'
);
```

### Scenario C: Querying Without Time Filter

**Problem**: Need to query all data for a specific entity without time bounds.

```sql
-- This query scans ALL chunks:
SELECT * FROM metrics WHERE device_id = 'sensor-42';  -- ğŸ˜± Full scan!
```

**Solutions**:

1. **Enable chunk skipping** (requires index on device_id):
```sql
-- Enable skip scan for non-time columns
SELECT enable_chunk_skipping('metrics', 'device_id');
```

2. **Space partitioning** (for very large single-entity datasets):
```sql
SELECT add_dimension('metrics', 'device_id', number_partitions => 16);
```

3. **Materialized view** (for dashboard queries):
```sql
CREATE MATERIALIZED VIEW latest_by_device AS
SELECT DISTINCT ON (device_id) *
FROM metrics
ORDER BY device_id, time DESC;
```

### Scenario D: Late-Arriving Data

**Problem**: Data arrives days or weeks late, after chunks are compressed.

```sql
-- This will decompress, insert, then compress again:
INSERT INTO metrics (time, device_id, value)
VALUES ('2024-01-01', 'sensor-1', 42.0);  -- Old data
```

**Solutions**:

1. **Backfill window in compression policy**:
```sql
ALTER TABLE metrics SET (
    timescaledb.compress_after = '7 days'  -- Give time for late data
);
```

2. **Manual chunk management for large backfills**:
```sql
-- Decompress specific chunks before bulk insert
SELECT decompress_chunk(c.chunk_schema || '.' || c.chunk_name)
FROM timescaledb_information.chunks c
WHERE c.hypertable_name = 'metrics'
  AND c.range_start >= '2024-01-01'
  AND c.range_end < '2024-01-08';

-- Bulk insert
\copy metrics FROM 'late_data.csv' CSV HEADER;

-- Recompress
SELECT compress_chunk(c.chunk_schema || '.' || c.chunk_name)
FROM timescaledb_information.chunks c
WHERE c.hypertable_name = 'metrics'
  AND c.range_start >= '2024-01-01'
  AND c.range_end < '2024-01-08'
  AND NOT c.is_compressed;
```

### Scenario E: Data That's Almost Time-Series

**Problem**: Data has timestamps but primary access pattern is not time-based.

Examples:
- User session data (query by user_id mostly)
- Product catalog with price history (query by product_id)
- Document versions (query by document_id)

**Decision Framework**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Primary query pattern?                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                               â”‚                             â”‚
â”‚  By entity (user, product)    â”‚    By time range            â”‚
â”‚                               â”‚                             â”‚
â”‚           â”‚                   â”‚          â”‚                  â”‚
â”‚           â–¼                   â”‚          â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Consider regular  â”‚       â”‚  â”‚   Use hypertable   â”‚     â”‚
â”‚  â”‚  table with time   â”‚       â”‚  â”‚   with entity in   â”‚     â”‚
â”‚  â”‚  column indexed    â”‚       â”‚  â”‚   segmentby        â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                               â”‚                             â”‚
â”‚  UNLESS you need:             â”‚                             â”‚
â”‚  â€¢ Automatic retention        â”‚                             â”‚
â”‚  â€¢ Compression (>90%)         â”‚                             â”‚
â”‚  â€¢ Time-range analytics       â”‚                             â”‚
â”‚                               â”‚                             â”‚
â”‚  Then: hypertable with        â”‚                             â”‚
â”‚  entity_id as segmentby       â”‚                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Multi-Tenant Architecture Patterns

### Pattern 1: Shared Database, Tenant Column + RLS

**Best for**: Many tenants (100+), similar data volumes, cost-sensitive

```sql
CREATE TABLE metrics (
    time TIMESTAMPTZ NOT NULL,
    tenant_id UUID NOT NULL,
    device_id TEXT NOT NULL,
    value DOUBLE PRECISION
) WITH (
    timescaledb.hypertable,
    timescaledb.partition_column = 'time',
    timescaledb.segmentby = 'tenant_id'  -- Critical!
);

-- Row-Level Security
ALTER TABLE metrics ENABLE ROW LEVEL SECURITY;

CREATE POLICY tenant_policy ON metrics
    USING (tenant_id = current_setting('app.tenant_id')::UUID);

-- Application sets context:
-- SET app.tenant_id = 'tenant-uuid';
```

### Pattern 2: Schema-Per-Tenant

**Best for**: Moderate tenants (10-100), need logical separation, different retention needs

```sql
-- Create schema per tenant
CREATE SCHEMA tenant_acme;

CREATE TABLE tenant_acme.metrics (
    time TIMESTAMPTZ NOT NULL,
    device_id TEXT NOT NULL,
    value DOUBLE PRECISION
);
SELECT create_hypertable('tenant_acme.metrics', 'time');

-- Repeat for each tenant, or automate:
CREATE OR REPLACE FUNCTION create_tenant_schema(tenant_name TEXT)
RETURNS VOID AS $$
BEGIN
    EXECUTE format('CREATE SCHEMA IF NOT EXISTS %I', tenant_name);
    EXECUTE format('
        CREATE TABLE %I.metrics (
            time TIMESTAMPTZ NOT NULL,
            device_id TEXT NOT NULL,
            value DOUBLE PRECISION
        )', tenant_name);
    EXECUTE format('SELECT create_hypertable(''%I.metrics'', ''time'')', tenant_name);
END;
$$ LANGUAGE plpgsql;
```

### Pattern 3: Database-Per-Tenant

**Best for**: Enterprise tenants, strict isolation requirements, compliance needs

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PostgreSQL Cluster                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   db_tenant_a   â”‚   db_tenant_b   â”‚      db_tenant_c        â”‚
â”‚   (TimescaleDB) â”‚   (TimescaleDB) â”‚      (TimescaleDB)      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚ metrics  â”‚  â”‚   â”‚ metrics  â”‚  â”‚      â”‚ metrics  â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Application routes connections based on tenant.

### Comparison Matrix

| Factor | Shared + RLS | Schema-Per-Tenant | DB-Per-Tenant |
|--------|--------------|-------------------|---------------|
| Number of tenants | 100s-1000s | 10s-100s | 1s-10s |
| Isolation level | Logical | Logical | Physical |
| Resource sharing | Maximum | Moderate | None |
| Management overhead | Low | Moderate | High |
| Per-tenant backup | Difficult | Moderate | Easy |
| Per-tenant retention | Via policy | Native | Native |
| Compliance (HIPAA, etc.) | Possible | Better | Best |
| Cost efficiency | Best | Good | Expensive |

---

## 6. TimescaleDB vs Alternatives Decision Guide

### When to Choose TimescaleDB

```
âœ… CHOOSE TIMESCALEDB IF:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ You're already using PostgreSQL                           â”‚
â”‚ â€¢ You need relational features (JOINs, FK, transactions)    â”‚
â”‚ â€¢ You have mixed workload (OLTP + time-series)             â”‚
â”‚ â€¢ Team knows SQL (don't want new query language)           â”‚
â”‚ â€¢ Data requires updates/deletes (mutable time-series)      â”‚
â”‚ â€¢ You need PostGIS for geospatial + time-series            â”‚
â”‚ â€¢ Moderate write throughput (<1M rows/sec sustained)       â”‚
â”‚ â€¢ You value PostgreSQL ecosystem (tools, extensions)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### When to Consider ClickHouse Instead

```
âš¡ CONSIDER CLICKHOUSE IF:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Purely analytical workload (no transactions needed)       â”‚
â”‚ â€¢ Massive write throughput (10M+ rows/sec)                 â”‚
â”‚ â€¢ Petabyte-scale data                                      â”‚
â”‚ â€¢ Complex aggregations over billions of rows               â”‚
â”‚ â€¢ Data is immutable (no updates/deletes)                   â”‚
â”‚ â€¢ Willing to learn ClickHouse SQL dialect                  â”‚
â”‚ â€¢ Don't need relational features                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### When to Consider InfluxDB Instead

```
ğŸ“Š CONSIDER INFLUXDB IF:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Pure metrics/monitoring use case                          â”‚
â”‚ â€¢ High-frequency individual writes (not batched)           â”‚
â”‚ â€¢ Native Flux language is acceptable                        â”‚
â”‚ â€¢ Need built-in downsampling out of box                    â”‚
â”‚ â€¢ Integrating with Telegraf ecosystem                      â”‚
â”‚ â€¢ Don't need relational features at all                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Hybrid Architecture Pattern

The "smart factory" pattern combines multiple databases for their strengths:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA PIPELINE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Sensors â”‚â”€â”€â”€â”€â–¶â”‚  Kafka  â”‚â”€â”€â”€â”€â–¶â”‚                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â”‚                     â”‚   â”‚
â”‚                       â”‚          â”‚                     â”‚   â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                     â”‚   â”‚
â”‚       â”‚               â”‚          â”‚                     â”‚   â”‚
â”‚       â–¼               â–¼          â–¼                     â”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚InfluxDB â”‚    â”‚TimescaleDBâ”‚   â”‚    ClickHouse     â”‚  â”‚   â”‚
â”‚  â”‚(alerts) â”‚    â”‚(OLTP+OLAP)â”‚   â”‚ (historical OLAP) â”‚  â”‚   â”‚
â”‚  â”‚<1 hour  â”‚    â”‚<30 days   â”‚   â”‚    >30 days       â”‚  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚       â”‚               â”‚                  â”‚              â”‚   â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
â”‚                       â–¼                                 â”‚   â”‚
â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚   â”‚
â”‚               â”‚   Grafana    â”‚                          â”‚   â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. Migration Decision Trees

### Migrating FROM PostgreSQL to TimescaleDB

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          PostgreSQL â†’ TimescaleDB Migration                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                               â”‚
      Table is empty               Table has data
            â”‚                               â”‚
            â–¼                               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚CREATE TABLE   â”‚             â”‚ How much data?      â”‚
    â”‚WITH (tsdb..)  â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚                           â”‚
                         < 100GB                       > 100GB
                              â”‚                           â”‚
                              â–¼                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚create_hypertableâ”‚        â”‚ Staged migration:   â”‚
                    â”‚migrate_data=trueâ”‚        â”‚ 1. Create new table â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ 2. Copy in chunks   â”‚
                                               â”‚ 3. Rename tables    â”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Migrating FROM InfluxDB to TimescaleDB

```sql
-- 1. Create equivalent schema
CREATE TABLE metrics (
    time TIMESTAMPTZ NOT NULL,
    measurement TEXT NOT NULL,  -- InfluxDB measurement name
    tags JSONB NOT NULL,        -- InfluxDB tags as JSONB
    fields JSONB NOT NULL       -- InfluxDB fields as JSONB
);
SELECT create_hypertable('metrics', 'time');

-- 2. Or normalized schema (better for querying):
CREATE TABLE metrics (
    time TIMESTAMPTZ NOT NULL,
    metric_name TEXT NOT NULL,
    host TEXT,
    region TEXT,
    value DOUBLE PRECISION
);
SELECT create_hypertable('metrics', 'time');

-- 3. Export from InfluxDB using influx CLI, import via COPY
```

---

## 8. Performance Optimization Decision Framework

### Query Performance Decision Tree

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Query is slow. What to do?                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Does query have time  â”‚
                â”‚ filter/constraint?    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                               â”‚
           NO                              YES
            â”‚                               â”‚
            â–¼                               â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ADD TIME FILTER!    â”‚      â”‚ Check EXPLAIN ANALYZE      â”‚
   â”‚This is #1 mistake  â”‚      â”‚ Is it scanning all chunks? â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â”‚                           â”‚
                         Scanning all            Only relevant chunks
                               â”‚                           â”‚
                               â–¼                           â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Missing index?     â”‚     â”‚ Need better index?     â”‚
                   â”‚ â€¢ Add (col, time)  â”‚     â”‚ â€¢ Check column order   â”‚
                   â”‚ â€¢ Enable chunk skipâ”‚     â”‚ â€¢ Consider BRIN        â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ â€¢ Partial index        â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Compression Decision Framework

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Compression not working well?                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Check compression ratio   â”‚
                â”‚ SELECT * FROM             â”‚
                â”‚ chunk_compression_stats   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                                       â”‚
        < 50%                                    > 70%
    (poor compression)                      (good compression)
            â”‚                                       â”‚
            â–¼                                       â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Check segmentby config  â”‚          â”‚ âœ… Compression OK   â”‚
   â”‚                         â”‚          â”‚ Consider:           â”‚
   â”‚ High cardinality in     â”‚          â”‚ â€¢ More aggressive   â”‚
   â”‚ segmentby?              â”‚          â”‚   compress_after    â”‚
   â”‚                         â”‚          â”‚ â€¢ Bloom filters     â”‚
   â”‚ Move high-cardinality   â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚ to orderby instead      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Chunk Sizing Decision

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             What chunk interval should I use?                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RULE: Chunk indexes should fit in 25% of RAM              â”‚
â”‚                                                             â”‚
â”‚  Formula:                                                   â”‚
â”‚  Index growth per day Ã— chunk_days â‰¤ 25% of RAM            â”‚
â”‚                                                             â”‚
â”‚  Example: 64GB RAM â†’ 16GB for chunk indexes                 â”‚
â”‚           10GB index growth/day â†’ use 1-day chunks          â”‚
â”‚           2GB index growth/day â†’ use 7-day chunks           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Typical starting points by use case:                       â”‚
â”‚                                                             â”‚
â”‚  IoT (high volume):         4 hours - 1 day                â”‚
â”‚  Metrics/Observability:     1 day - 1 week                 â”‚
â”‚  Financial tick data:       1 hour - 4 hours               â”‚
â”‚  Event logs:                1 day - 1 week                 â”‚
â”‚  Slow-moving data:          1 week - 1 month               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Quick Reference Cards

### Hypertable Checklist

```
Before creating a hypertable:

â–¡ Time column is TIMESTAMPTZ (not TIMESTAMP)
â–¡ Time column cannot be NULL
â–¡ Unique constraints include time column
â–¡ Foreign keys FROM hypertable (not TO) are planned
â–¡ Chunk interval calculated based on index size + RAM
â–¡ segmentby columns have reasonable cardinality (<10K values)
â–¡ No plans for cross-chunk UPDATEs
```

### Compression Checklist

```
Before enabling compression:

â–¡ segmentby: Low cardinality columns used for filtering
â–¡ orderby: Time column + high-cardinality columns
â–¡ compress_after: Long enough for late-arriving data
â–¡ Bloom filters considered for high-cardinality lookups
â–¡ Tested that INSERT/UPDATE patterns work with compression
```

### Continuous Aggregate Checklist

```
Before creating continuous aggregates:

â–¡ WITH NO DATA for large existing datasets
â–¡ Appropriate bucket size for query patterns
â–¡ Refresh policy matches data freshness needs
â–¡ Consider hierarchical aggregates (minute â†’ hour â†’ day)
â–¡ Include all GROUP BY columns needed for RLS
```

---

*This guide is a companion to the TimescaleDB Master Guide 2026. For detailed technical reference, consult the Master Guide and official documentation.*
